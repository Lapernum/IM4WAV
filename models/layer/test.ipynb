{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import torch\n",
    "import matplotlib\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image\n",
    "from depth import getDepthHead\n",
    "from segmentation import getSegmentationHead, getSegmentationModel\n",
    "from mmseg.apis import inference_segmentor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = Image.open(r\"S:/CS7643Project/audiosetdl/dataset/image/3-9.#alligators, crocodiles hissing#.train.Alligators Blackwater & Thrasher!  VLOGMAS Day 15.jpg\")\n",
    "display(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_depth_transform() -> transforms.Compose:\n",
    "    return transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        lambda x: 255.0 * x[:3], # Discard alpha component and scale by 255\n",
    "        transforms.Normalize(\n",
    "            mean=(123.675, 116.28, 103.53),\n",
    "            std=(58.395, 57.12, 57.375),\n",
    "        ),\n",
    "        transforms.Resize((240, 320)),\n",
    "    ])\n",
    "\n",
    "\n",
    "def render_depth(values, colormap_name=\"magma_r\") -> Image:\n",
    "    min_value, max_value = values.min(), values.max()\n",
    "    normalized_values = (values - min_value) / (max_value - min_value)\n",
    "\n",
    "    colormap = matplotlib.colormaps[colormap_name]\n",
    "    colors = colormap(normalized_values, bytes=True) # ((1)xhxwx4)\n",
    "    colors = colors[:, :, :3] # Discard alpha component\n",
    "    return Image.fromarray(colors)\n",
    "\n",
    "\n",
    "transform = make_depth_transform()\n",
    "\n",
    "scale_factor = 1\n",
    "rescaled_image = test_image.resize((scale_factor * test_image.width, scale_factor * test_image.height))\n",
    "transformed_image = transform(rescaled_image)\n",
    "batch = transformed_image.unsqueeze(0).cuda() # Make a batch of one image\n",
    "\n",
    "model = getDepthHead().cuda()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    result = model.whole_inference(batch, img_meta=None, rescale=True)\n",
    "\n",
    "depth_image = render_depth(result.squeeze().cpu())\n",
    "display(depth_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import model.dinov2.eval.segmentation.utils.colormaps as colormaps\n",
    "\n",
    "\n",
    "DATASET_COLORMAPS = {\n",
    "    \"ade20k\": colormaps.ADE20K_COLORMAP,\n",
    "    \"voc2012\": colormaps.VOC2012_COLORMAP,\n",
    "}\n",
    "\n",
    "HEAD_DATASET = \"voc2012\"\n",
    "\n",
    "def make_segmentation_transform() -> transforms.Compose:\n",
    "    return transforms.Compose([\n",
    "        # transforms.ToTensor(),\n",
    "        # lambda x: 255.0 * x[:3], # Discard alpha component and scale by 255\n",
    "        # transforms.Normalize(\n",
    "        #     mean=(123.675, 116.28, 103.53),\n",
    "        #     std=(58.395, 57.12, 57.375),\n",
    "        # ),\n",
    "        transforms.Resize((240, 320)),\n",
    "    ])\n",
    "\n",
    "def render_segmentation(segmentation_logits, dataset):\n",
    "    colormap = DATASET_COLORMAPS[dataset]\n",
    "    colormap_array = np.array(colormap, dtype=np.uint8)\n",
    "    segmentation_logits[segmentation_logits + 1 >= colormap_array.shape[0]] = colormap_array.shape[0] - 2\n",
    "    segmentation_values = colormap_array[segmentation_logits + 1]\n",
    "    return Image.fromarray(segmentation_values)\n",
    "\n",
    "transform = make_segmentation_transform()\n",
    "\n",
    "transformed_image = transform(test_image)\n",
    "\n",
    "seg_model = getSegmentationModel()\n",
    "\n",
    "array = np.array(transformed_image)[:, :, ::-1] # BGR\n",
    "segmentation_logits = inference_segmentor(seg_model, array)[0]\n",
    "segmented_image = render_segmentation(segmentation_logits, HEAD_DATASET)\n",
    "display(segmented_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_transform() -> transforms.Compose:\n",
    "    return transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "tensor_transformer = tensor_transform()\n",
    "transformed_segmented_image = np.array(tensor_transformer(segmented_image)).transpose((1, 2, 0))\n",
    "transformed_depth_image = np.array(tensor_transformer(depth_image)).transpose((1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_segmented_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_depth_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray_segmented_image = cv2.cvtColor(transformed_segmented_image, cv2.COLOR_BGR2GRAY)\n",
    "cv2.imshow(\"image\", gray_segmented_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray_depth_image = cv2.cvtColor(transformed_depth_image, cv2.COLOR_BGR2GRAY)\n",
    "cv2.imshow(\"image\", gray_depth_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounded_segmented_image = np.around(gray_segmented_image, decimals=2)\n",
    "cv2.imshow(\"image\", rounded_segmented_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounded_depth_image = np.around(gray_depth_image, decimals=2)\n",
    "cv2.imshow(\"image\", rounded_depth_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray_segmented_image==0.44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gray_image = np.copy(rounded_segmented_image)\n",
    "test_gray_image[test_gray_image!=0.47] = 0\n",
    "test_gray_image[test_gray_image==0.47] = 1\n",
    "cv2.imshow(\"image\", test_gray_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_gray_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def original_transform() -> transforms.Compose:\n",
    "    return transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((240, 320)),\n",
    "    ])\n",
    "original_tranformer = original_transform()\n",
    "transformed_original_image = np.array(original_tranformer(test_image)).transpose((1, 2, 0))[:, :, ::-1]\n",
    "cv2.imshow(\"image\", transformed_original_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_original_image = cv2.imread(r\"S:/CS7643Project/audiosetdl/dataset/image/3-9.#alligators, crocodiles hissing#.train.Alligators Blackwater & Thrasher!  VLOGMAS Day 15.jpg\")\n",
    "cv_original_image = cv2.resize(cv_original_image, (320, 240))\n",
    "cv2.imshow(\"image\", cv_original_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROI_number = 0\n",
    "volume_factor = []\n",
    "for gray_scale in list(np.array(range(101)) / 100.0):\n",
    "    # Morph open to remove noise\n",
    "    test_gray_image = np.copy(rounded_segmented_image)\n",
    "    test_gray_image[test_gray_image!=gray_scale] = 0\n",
    "    test_gray_image[test_gray_image==gray_scale] = 1\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5,5))\n",
    "    opening = cv2.morphologyEx(test_gray_image, cv2.MORPH_OPEN, kernel, iterations=1).astype('uint8')\n",
    "\n",
    "    masked_depth_image = np.multiply(gray_depth_image, test_gray_image)\n",
    "\n",
    "    # Find contours, obtain bounding box, extract and save ROI\n",
    "    cnts = cv2.findContours(opening, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "    for idx, c in enumerate(cnts):\n",
    "        x,y,w,h = cv2.boundingRect(c)\n",
    "        # cv2.rectangle(cv_original_image, (x, y), (x + w, y + h), (36,255,12), 0.1)\n",
    "        if w < 32 or h < 32:\n",
    "            continue\n",
    "        c_mask = np.zeros((240, 320), np.uint8)\n",
    "        cv2.drawContours(c_mask, cnts, idx, 255, -1)\n",
    "        # ROI_depth = masked_depth_image[y:y+h, x:x+w]\n",
    "        ROI_depth = masked_depth_image[c_mask == 255]\n",
    "        ROI_depth_mean = np.mean(ROI_depth)\n",
    "        ROI_depth_factor = ROI_depth_mean ** 2\n",
    "\n",
    "        ROI_horizontal_ratio = ((x + 0.5 * w) - 160) / 320.0\n",
    "        HALF_ANGLE_TAN = math.tan(5 * math.pi / 36)\n",
    "        ROI_horizontal_angle = math.atan(2 * abs(ROI_horizontal_ratio) * HALF_ANGLE_TAN)\n",
    "\n",
    "        if ROI_horizontal_ratio < 0:\n",
    "            ROI_horizontal_factor_leftC = math.cos(((0.5 * math.pi) - ROI_horizontal_angle) / 2)\n",
    "            ROI_horizontal_factor_rightC = math.sin(((0.5 * math.pi) - ROI_horizontal_angle) / 2)\n",
    "        else:\n",
    "            ROI_horizontal_factor_leftC = math.cos(((0.5 * math.pi) + ROI_horizontal_angle) / 2)\n",
    "            ROI_horizontal_factor_rightC = math.sin(((0.5 * math.pi) + ROI_horizontal_angle) / 2)\n",
    "        \n",
    "        volume_factor.append((0.5 * ROI_depth_factor + 0.5 * ROI_horizontal_factor_leftC, 0.5 * ROI_depth_factor + 0.5 * ROI_horizontal_factor_rightC))\n",
    "        ROI = cv_original_image[y:y+h, x:x+w]\n",
    "        cv2.imwrite('test_images/ROI-{}.png'.format(ROI_number), ROI)\n",
    "        ROI_number += 1\n",
    "\n",
    "    # cv2.imshow('image', cv_original_image)\n",
    "    # cv2.imshow('thresh', test_gray_image)\n",
    "    # cv2.imshow('opening', opening)\n",
    "    # cv2.waitKey()\n",
    "\n",
    "print(volume_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "detection_model = YOLO('yolov8m.pt')\n",
    "results = detection_model(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

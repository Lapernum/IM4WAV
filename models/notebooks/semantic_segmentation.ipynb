{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b470389d-a897-416e-9601-aeacb39cd694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5c8577-7dff-41b1-9b04-2dca12940e02",
   "metadata": {},
   "source": [
    "# Semantic Segmentation <a target=\"_blank\" href=\"https://colab.research.google.com/github/facebookresearch/dinov2/blob/main/notebooks/semantic_segmentation.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febdf412-5ad0-4bbc-9530-754f92dcc491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "INSTALL = False # Switch this to install dependencies\n",
    "if INSTALL: # Try installing package with extras\n",
    "    REPO_URL = \"https://github.com/facebookresearch/dinov2\"\n",
    "    !{sys.executable} -m pip install -e {REPO_URL}'[extras]' --extra-index-url https://download.pytorch.org/whl/cu117  --extra-index-url https://pypi.nvidia.com\n",
    "else:\n",
    "    REPO_PATH = \"<FIXME>\" # Specify a local path to the repository (or use installed package instead)\n",
    "    sys.path.append(REPO_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdf378b-0591-4879-9db6-6a4ab582d49f",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90223c04-e7da-4738-bb16-d4f7025aa3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import itertools\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from mmseg.apis import init_segmentor, inference_segmentor\n",
    "\n",
    "import sys,os\n",
    "sys.path.append(\"D:/CS7643/project/ImageToAudio/model/\")\n",
    "\n",
    "import dinov2.eval.segmentation.models\n",
    "\n",
    "\n",
    "class CenterPadding(torch.nn.Module):\n",
    "    def __init__(self, multiple):\n",
    "        super().__init__()\n",
    "        self.multiple = multiple\n",
    "\n",
    "    def _get_pad(self, size):\n",
    "        new_size = math.ceil(size / self.multiple) * self.multiple\n",
    "        pad_size = new_size - size\n",
    "        pad_size_left = pad_size // 2\n",
    "        pad_size_right = pad_size - pad_size_left\n",
    "        return pad_size_left, pad_size_right\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def forward(self, x):\n",
    "        pads = list(itertools.chain.from_iterable(self._get_pad(m) for m in x.shape[:1:-1]))\n",
    "        output = F.pad(x, pads)\n",
    "        return output\n",
    "\n",
    "\n",
    "def create_segmenter(cfg, backbone_model):\n",
    "    model = init_segmentor(cfg)\n",
    "    model.backbone.forward = partial(\n",
    "        backbone_model.get_intermediate_layers,\n",
    "        n=cfg.model.backbone.out_indices,\n",
    "        reshape=True,\n",
    "    )\n",
    "    if hasattr(backbone_model, \"patch_size\"):\n",
    "        model.backbone.register_forward_pre_hook(lambda _, x: CenterPadding(backbone_model.patch_size)(x[0]))\n",
    "    model.init_weights()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5724efc-b2b8-46ed-94e1-7fee59a39ed9",
   "metadata": {},
   "source": [
    "## Load pretrained backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d51b932-1157-45ce-997f-572ad417a12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKBONE_SIZE = \"small\" # in (\"small\", \"base\", \"large\" or \"giant\")\n",
    "\n",
    "\n",
    "backbone_archs = {\n",
    "    \"small\": \"vits14\",\n",
    "    \"base\": \"vitb14\",\n",
    "    \"large\": \"vitl14\",\n",
    "    \"giant\": \"vitg14\",\n",
    "}\n",
    "backbone_arch = backbone_archs[BACKBONE_SIZE]\n",
    "backbone_name = f\"dinov2_{backbone_arch}\"\n",
    "\n",
    "backbone_model = torch.hub.load(repo_or_dir=\"facebookresearch/dinov2\", model=backbone_name)\n",
    "backbone_model.eval()\n",
    "backbone_model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c90501-d6ef-436e-b1a1-72e63b0534e3",
   "metadata": {},
   "source": [
    "## Load pretrained segmentation head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bf0b7f-ad98-4cfb-8120-f076df8f8933",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "import mmcv\n",
    "from mmcv.runner import load_checkpoint\n",
    "\n",
    "\n",
    "def load_config_from_url(url: str) -> str:\n",
    "    with urllib.request.urlopen(url) as f:\n",
    "        return f.read().decode()\n",
    "\n",
    "\n",
    "HEAD_SCALE_COUNT = 3 # more scales: slower but better results, in (1,2,3,4,5)\n",
    "HEAD_DATASET = \"voc2012\" # in (\"ade20k\", \"voc2012\")\n",
    "HEAD_TYPE = \"ms\" # in (\"ms, \"linear\")\n",
    "\n",
    "\n",
    "DINOV2_BASE_URL = \"https://dl.fbaipublicfiles.com/dinov2\"\n",
    "head_config_url = f\"{DINOV2_BASE_URL}/{backbone_name}/{backbone_name}_{HEAD_DATASET}_{HEAD_TYPE}_config.py\"\n",
    "head_checkpoint_url = f\"{DINOV2_BASE_URL}/{backbone_name}/{backbone_name}_{HEAD_DATASET}_{HEAD_TYPE}_head.pth\"\n",
    "\n",
    "cfg_str = load_config_from_url(head_config_url)\n",
    "cfg = mmcv.Config.fromstring(cfg_str, file_format=\".py\")\n",
    "if HEAD_TYPE == \"ms\":\n",
    "    cfg.data.test.pipeline[1][\"img_ratios\"] = cfg.data.test.pipeline[1][\"img_ratios\"][:HEAD_SCALE_COUNT]\n",
    "    print(\"scales:\", cfg.data.test.pipeline[1][\"img_ratios\"])\n",
    "\n",
    "model = create_segmenter(cfg, backbone_model=backbone_model)\n",
    "load_checkpoint(model, head_checkpoint_url, map_location=\"cpu\")\n",
    "model.cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc1b106-d28c-41cc-9ddd-f558d66a4715",
   "metadata": {},
   "source": [
    "## Load sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44511634-8243-4662-a512-4531014adb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def load_image_from_url(url: str) -> Image:\n",
    "    with urllib.request.urlopen(url) as f:\n",
    "        return Image.open(f).convert(\"RGB\")\n",
    "\n",
    "\n",
    "EXAMPLE_IMAGE_URL = \"https://dl.fbaipublicfiles.com/dinov2/images/example.jpg\"\n",
    "\n",
    "\n",
    "image = load_image_from_url(EXAMPLE_IMAGE_URL)\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3240cb-54d0-438d-99e8-8c1af534f830",
   "metadata": {},
   "source": [
    "## Semantic segmentation on sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49226d5b-83fc-4cfb-ba06-407bb2c0d96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import dinov2.eval.segmentation.utils.colormaps as colormaps\n",
    "\n",
    "\n",
    "DATASET_COLORMAPS = {\n",
    "    \"ade20k\": colormaps.ADE20K_COLORMAP,\n",
    "    \"voc2012\": colormaps.VOC2012_COLORMAP,\n",
    "}\n",
    "\n",
    "\n",
    "def render_segmentation(segmentation_logits, dataset):\n",
    "    colormap = DATASET_COLORMAPS[dataset]\n",
    "    colormap_array = np.array(colormap, dtype=np.uint8)\n",
    "    segmentation_values = colormap_array[segmentation_logits + 1]\n",
    "    return Image.fromarray(segmentation_values)\n",
    "\n",
    "\n",
    "array = np.array(image)[:, :, ::-1] # BGR\n",
    "segmentation_logits = inference_segmentor(model, array)[0]\n",
    "segmented_image = render_segmentation(segmentation_logits, HEAD_DATASET)\n",
    "display(segmented_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de40012e-a01e-4e73-bb71-3048f16d41c8",
   "metadata": {},
   "source": [
    "## Load pretrained segmentation model (Mask2Former)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2cbbbe-c53c-4e5b-977f-c2a7d93f4b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dinov2.eval.segmentation_m2f.models.segmentors\n",
    "\n",
    "CONFIG_URL = f\"{DINOV2_BASE_URL}/dinov2_vitg14/dinov2_vitg14_ade20k_m2f_config.py\"\n",
    "CHECKPOINT_URL = f\"{DINOV2_BASE_URL}/dinov2_vitg14/dinov2_vitg14_ade20k_m2f.pth\"\n",
    "\n",
    "cfg_str = load_config_from_url(CONFIG_URL)\n",
    "cfg = mmcv.Config.fromstring(cfg_str, file_format=\".py\")\n",
    "\n",
    "model = init_segmentor(cfg)\n",
    "load_checkpoint(model, CHECKPOINT_URL, map_location=\"cpu\")\n",
    "model.cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c0309f-df2b-4912-bca5-e57d8b3875b3",
   "metadata": {},
   "source": [
    "## Semantic segmentation on sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4abb13b-0e5a-4a40-8d44-21da4286ba7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = np.array(image)[:, :, ::-1] # BGR\n",
    "segmentation_logits = inference_segmentor(model, array)[0]\n",
    "segmented_image = render_segmentation(segmentation_logits, \"ade20k\")\n",
    "display(segmented_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb1db5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
